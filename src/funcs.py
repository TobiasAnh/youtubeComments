#!/usr/bin/env python3
import os
import traceback
import json
import re
import pandas as pd
from pathlib import Path
from datetime import datetime
from src.api import setupYouTube

datetime.today()


class ytChannel:
    def __init__(self, channelId):
        """
        Requests basic channel metrics using a YouTube Channel id. Stores relevant channel info
        as attributes such as self.playlistId
                Parameters:
                        channelId (string): valid YouTube channelId.

                Returns:
                        No returns.

        """

        validChannelId = bool(re.match(r"^[a-zA-Z0-9_-]{24}$", channelId))
        if not validChannelId:
            print("No valid YouTube channel ID provided.")
            return
        else:
            self.channelId = channelId

        # Request channel infos
        youtube = setupYouTube()
        channel_request = youtube.channels().list(
            part="snippet, statistics, contentDetails", id=self.channelId
        )
        channel_response = channel_request.execute()

        # Store channel metrics in dictionary
        self.channelTitle = channel_response["items"][0]["snippet"]["title"]
        self.publishedAt = pd.to_datetime(
            channel_response["items"][0]["snippet"]["publishedAt"]
        )
        self.playlistId = channel_response["items"][0]["contentDetails"][
            "relatedPlaylists"
        ]["uploads"]
        self.channel_foldername = self.channelTitle.replace(" ", "_").replace("&", "")
        self.statistics = channel_response["items"][0]["statistics"]

        print(f"Loaded YT Channel: {self.channelTitle}")

        # Generate subfolders in repository. Used later for storage of data, mostly .csv files

        # Repository path
        self.project_path = Path(os.getcwd())

        # data path
        self.data_path = self.project_path.joinpath("data")
        self.data_path.mkdir(exist_ok=True, parents=True)

        # Subfolders
        self.interim_path = self.data_path.joinpath("interim")
        self.processed_path = self.data_path.joinpath("processed")
        self.reports_path = self.data_path.joinpath("reports")

        for subfolder in ["interim", "processes", "reports"]:
            self.data_path.joinpath(subfolder).mkdir(exist_ok=True, parents=True)

        # Subsubfolder for chosen channel
        self.channel_path = self.interim_path.joinpath(self.channel_foldername)
        self.channel_path.mkdir(exist_ok=True, parents=True)

        print("Folders generated in ... ")
        print(self.data_path)
        print()

    def getVideoIds(self):
        """
        Requests basic video information from Youtube playlistId.
        Assign DataFrame to object attribute (required for further analysis)

        """

        # Instantiate youtube instance
        youtube = setupYouTube()

        # Data frame populated in while loop below
        raw_video_info = pd.DataFrame(
            columns=[
                "videoId",
                "Title",
                "playlistId",
                "videoOwnerChannelId",
                "videoOwnerChannelTitle",
            ]
        )

        # Loop breaks when no nextPageToken is generated
        nextPageToken = None
        while True:
            # Set up request
            videos_request = youtube.playlistItems().list(
                part="snippet",
                playlistId=self.playlistId,
                maxResults=50,
                pageToken=nextPageToken,
            )

            # Execute request
            videos_response = videos_request.execute()

            # Loop through "items" to get videoId and title
            for item in videos_response["items"]:
                video = dict()
                video["videoId"] = item["snippet"]["resourceId"]["videoId"]
                video["Title"] = item["snippet"]["title"]
                video["playlistId"] = item["snippet"]["playlistId"]

                # Below required since these information are not existent when ...
                # ... videos are "private"
                if "videoOwnerChannelId" in item["snippet"]:
                    video["videoOwnerChannelId"] = item["snippet"][
                        "videoOwnerChannelId"
                    ]
                    video["videoOwnerChannelTitle"] = item["snippet"][
                        "videoOwnerChannelTitle"
                    ]

                _ = len(raw_video_info)
                raw_video_info.loc[_] = video

            nextPageToken = videos_response.get("nextPageToken")

            if not nextPageToken:
                break

        self.raw_video_info = raw_video_info

        print(
            f'{len(raw_video_info)} videos found for {video["videoOwnerChannelTitle"]}'
        )
        print()

    def getAndSaveVideoStatistics(self, check_existing=True):
        """
        Augments additional video metrics to DataFrame generated by getVideoIds().

                Parameters:
                        use_existing (bool): Use existing .csv file to continue analysis.
                        If no file found, fresh data is loaded.

                Returns:
                        No returns. video metrics are stored in /{channel_path}/all_videos.csv
        """

        # Import videoIds back from local storage
        if check_existing:
            try:
                all_videos = pd.read_csv(
                    self.channel_path.joinpath("all_videos.csv"),
                    index_col="videoId",
                    lineterminator="\r",
                    parse_dates=["publishedAt"],
                )
                all_videos["channel_foldername"] = self.channel_foldername

                print("Videos metrics already fetched and stored in all_videos.csv")
                print(f"Last video loaded {all_videos['publishedAt'].max()}")
                print("Set check_existing=False to fetch fresh API data")

                return
            except FileNotFoundError as e:
                print("No existing file 'all_videos.csv' found.")

        # Instantiate youtube instance with given api_key_selector
        youtube = setupYouTube()

        # Data frame filled in loop below
        all_metrics = pd.DataFrame(
            columns=[
                "viewCount",
                "likeCount",
                "commentCount",
                "duration",
                "definition",
                "publishedAt",
                "description",
                "categoryId",
                "videoId",
            ]
        )

        # Variables for loop reporting
        d = 0

        for counter, videoId in enumerate(self.raw_video_info["videoId"]):
            video_request = youtube.videos().list(
                part="snippet, contentDetails, statistics", id=videoId
            )

            video_response = video_request.execute()

            if video_response["items"]:
                metrics = dict()
                metrics = video_response["items"][0]["statistics"]
                metrics["duration"] = video_response["items"][0]["contentDetails"][
                    "duration"
                ]
                metrics["definition"] = video_response["items"][0]["contentDetails"][
                    "definition"
                ]
                metrics["publishedAt"] = pd.to_datetime(
                    video_response["items"][0]["snippet"]["publishedAt"]
                )
                metrics["description"] = video_response["items"][0]["snippet"][
                    "description"
                ]
                metrics["categoryId"] = video_response["items"][0]["snippet"][
                    "categoryId"
                ]
                metrics["videoId"] = video_response["items"][0]["id"]

            _ = len(all_metrics)
            all_metrics.loc[_] = metrics

            if counter > d * 10:
                print(f"Metrics added for over {d*10} videos")
                d += 1

        # Concat original dataframe with requested metrics. Save dataframe as .csv
        all_videos = pd.merge(left=self.raw_video_info, right=all_metrics, on="videoId")
        all_videos.set_index("videoId").to_csv(
            self.channel_path.joinpath("all_videos.csv"), lineterminator="\r"
        )
        self.dateOfVideoFetch = datetime.now().date().today()
        print(f"Generated 'all_videos.csv' in {self.channel_path}")

    def getAndSaveComments(self):
        """
        Requests YouTube video comments from videoId list. videoIds are stored in self.raw_video_info.
        Comments are stored in tmp/ as csv for each video.

        If fetch incomplete, "missing_videos.json" is stored locally.

        """

        print("Getting channel comments ... ")

        if not "missing_videos.json" in os.listdir(self.channel_path):
            all_videos = pd.read_csv(
                self.channel_path.joinpath("all_videos.csv"),
                index_col="videoId",
                lineterminator="\r",
            )

            videoIds = list(
                all_videos.query(
                    "commentCount.notnull()"
                )  # NULL when comments are disabled
                .query(
                    "commentCount != 0"
                )  # 0 comments written (includes also Livestreams)
                .index
            )

        else:
            with open(
                self.channel_path.joinpath("missing_videos.json"), "r"
            ) as filepath:
                videoIds = list(json.load(filepath))

            print("Continuing with missing_videos.json")

        columns_comments = [
            "videoId",
            "comment_id",
            "comment_author",
            "comment_likes",
            "comment_replies",
            "comment_published",
            "comment_update",
            "comment_string",
            "reply_id",
            "top_level_comment",
        ]

        youtube = setupYouTube()

        # Loop through videos
        for counter, videoId in enumerate(videoIds):
            video_comments = pd.DataFrame(columns=columns_comments)

            # Try/except part allows only to store a complete comment request per video
            nextPageToken = None
            try:
                while True:
                    comments_request = youtube.commentThreads().list(
                        part="replies, snippet",
                        videoId=videoId,
                        maxResults=50,
                        pageToken=nextPageToken,
                        textFormat="plainText",
                    )

                    comments_response = comments_request.execute()

                    # comments and replies
                    for item in comments_response["items"]:
                        comments = dict()
                        comments["videoId"] = item["snippet"]["videoId"]
                        comments["comment_id"] = item["snippet"]["topLevelComment"][
                            "id"
                        ]
                        comments["comment_author"] = item["snippet"]["topLevelComment"][
                            "snippet"
                        ]["authorDisplayName"]
                        comments["comment_likes"] = item["snippet"]["topLevelComment"][
                            "snippet"
                        ]["likeCount"]
                        comments["comment_replies"] = item["snippet"]["totalReplyCount"]
                        comments["comment_published"] = item["snippet"][
                            "topLevelComment"
                        ]["snippet"]["publishedAt"]
                        comments["comment_update"] = item["snippet"]["topLevelComment"][
                            "snippet"
                        ]["updatedAt"]
                        comments["comment_string"] = item["snippet"]["topLevelComment"][
                            "snippet"
                        ]["textDisplay"]
                        comments["reply_id"] = "None"
                        comments["top_level_comment"] = True

                        _ = len(video_comments)
                        video_comments.loc[_] = comments

                        # Replies
                        if "replies" in item:
                            for reply in item["replies"]["comments"]:
                                replies = dict()
                                replies["videoId"] = reply["snippet"]["videoId"]
                                replies["comment_id"] = reply["snippet"]["parentId"]
                                replies["comment_author"] = reply["snippet"][
                                    "authorDisplayName"
                                ]
                                replies["comment_likes"] = reply["snippet"]["likeCount"]
                                replies["comment_replies"] = "NaN"  #
                                replies["comment_published"] = reply["snippet"][
                                    "publishedAt"
                                ]
                                replies["comment_update"] = reply["snippet"][
                                    "updatedAt"
                                ]
                                replies["comment_string"] = reply["snippet"][
                                    "textDisplay"
                                ]
                                replies["reply_id"] = reply["id"]  # parentId.replyId
                                replies["top_level_comment"] = False

                                _ = len(video_comments)
                                video_comments.loc[_] = replies

                    nextPageToken = comments_response.get("nextPageToken")
                    if not nextPageToken:
                        break

                # Save populated dataframe locally in temporary folder
                self.channel_path.joinpath("tmp").mkdir(exist_ok=True)

                video_comments.to_csv(
                    self.channel_path.joinpath("tmp", f"{videoId}.csv"),
                    escapechar="|",
                )

                progress = f"Video {counter+1} of {len(videoIds)}"
                print(f"{progress} {videoId} | {len(video_comments)} comments found")

            except Exception as e:
                # Code to handle any exception and display error details
                error_type = type(e).__name__
                error_msg = str(e)
                error_traceback = traceback.format_exc()
                print(f"Error: {error_type} - {error_msg}")
                print(f"Module/File: {error_traceback}")

        # Detect missing videos and save them locally in json
        _ = os.listdir(self.channel_path.joinpath("tmp"))
        fetched_videoIds = [os.path.splitext(x)[0] for x in _]
        missing_videos = list(set(videoIds).difference(set(fetched_videoIds)))

        if missing_videos:
            print(f"Comments fetch NOT complete. {len(missing_videos)} are missing.")
            with open(
                self.channel_path.joinpath("missing_videos.json"), "w"
            ) as filepath:
                json.dump(missing_videos, filepath)

            self.getAndSaveComments()

        # If no missing videos (fetch complete), remove missing_videos.json
        else:
            if "missing_videos.json" in os.listdir(self.channel_path):
                os.remove("missing_videos.json")
            print("Comments of all videos fetched :)")
            print()
            self._commentFetchComplete = True


def saveComments(self):
    # If final file already exists, do nothing, otherwise start OR continue comment fetch.
    if ("all_comments_noSentiment.csv" in os.listdir(self.channel_path)) or (
        "all_comments_withSentiment.csv" in os.listdir(self.channel_path)
    ):
        return "csv file found, Comment fetch appears finished"

    if self._commentFetchComplete == False:
        return "Comment fetch incomplete."

    print("Start concatination of .csv files")

    video_files = os.listdir(self.channel_path.joinpath("tmp"))
    all_comments = pd.DataFrame()

    for counter, file in enumerate(video_files):
        _ = pd.read_csv(
            self.channel_path.joinpath("tmp", file),
            index_col=0,
            parse_dates=["comment_published", "comment_update"],
            lineterminator="\n",
        )

        all_comments = pd.concat([all_comments, _], axis=0)

        progress = round(counter / len(video_files), 3)
        print(f"Fraction concatenated: {progress}")

    # Augment video features
    all_videos = pd.read_csv(
        self.channel_path.joinpath("all_videos.csv"),
        index_col="videoId",
        lineterminator="\r",
    )
    video_features = all_videos[["Title", "videoOwnerChannelTitle", "publishedAt"]]
    all_comments = pd.merge(
        left=all_comments, right=video_features, how="left", on="videoId"
    )

    # Drop unnecessary columns
    all_comments = all_comments.drop(["comment_update"], axis=1)


# =============================================================================
# Outsourced functions - No API requests involved below
# Mainly concatenations and data restructuring
# =============================================================================


def concatCommentsAndVideos(channel_paths):
    """
    Turns various csv files back into DataFrames.
    Requires two csv's "all_comments_withSentiment.csv" and "all_videos.csv"
    in each subfolder listed in channel_paths.

    Parameters:
            channel_paths (list): list of PosixPath's

    Returns:
            comments (DataFrame): concatenated comments found within channel_paths
            videos (DataFrame): concatenaed videos found within channel_paths
    """

    comments = pd.DataFrame()
    videos = pd.DataFrame()

    for channel_path in channel_paths:
        try:
            # Import all_comments...
            comments_per_channel = pd.read_csv(
                channel_path.joinpath("all_comments_withSentiment.csv"),
                index_col=0,
                lineterminator="\r",
                parse_dates=["publishedAt", "comment_published"],
            )

            comments = pd.concat([comments, comments_per_channel], axis=0)

            # Import all_videos
            videos_per_channel = pd.read_csv(
                channel_path.joinpath("all_videos.csv"),
                index_col=0,
                lineterminator="\r",
                parse_dates=["publishedAt"],
            )

            videos = pd.concat([videos, videos_per_channel], axis=0)

            print(f"comments and videos concatenated from {channel_path}")

        except FileNotFoundError:
            print("Required csv files not found in ... ")
            print(f"{channel_path}")

    return comments, videos


def findZDFurl(string):
    # TODO add description

    string = str(string).lower()

    # regex url detection
    regex = r"((?<=[^a-zA-Z0-9])(?:https?\:\/\/|[a-zA-Z0-9]{1,}\.{1}|\b)(?:\w{1,}\.{1}){1,5}(?:com|org|edu|gov|uk|net|ca|de|jp|fr|au|us|ru|ch|it|nl|se|no|es|mil|iq|io|ac|ly|sm){1}(?:\/[a-zA-Z0-9]{1,})*)"
    url = re.findall(regex, string)

    # return True when url references to ZDF (but not to the service section)
    if len(url) == 0:
        return False  # no url at all
    else:
        if str(url).find("zdf") != -1:
            if (
                str(url).find("service") != -1
            ):  # zdf url but reference to service site (netiquette)
                return False
            else:
                return True  # zdf url
        else:
            return False  # url but not ZDF


# =============================================================================
# Export and import of datatypes
# =============================================================================


def exportDFdtypes(df, jsonfile):
    """
    Exports data types from given data frame into json file
    See also importDFdtypes().

    Parameters:
            df (DataFrame): name of DataFrame
            jsonfile (str): Name of json file
    """

    df_dtypes = df.dtypes.astype(str).to_dict()

    with open(processed_path.joinpath(jsonfile), "w") as f:
        json.dump(df_dtypes, f)


def importDFdtypes(jsonfile):
    """
    ImoportExports data types from given data frame into json file
    See also exportDFdtypes().

    Parameters:
            jsonfile (str): Name of json file
    Return:
            dict() with datatypes

    """

    with open(processed_path.joinpath(jsonfile), "r") as f:
        return json.load(f)


# =============================================================================
# Other outsourced stuff
# =============================================================================

relabeling_dict = {
    "channelTitle": "YT-Kanal",
    "publishedAt": "Veröffentlicht am",
    "toplevel_sentiment_mean": "Sentiment-Index",
    "videoOwnerChannelTitle": "Kanal",
    "likeCount": "Likes",
    "likes_per_1kViews": "Beliebtheit (Likes pro 1000 Views)",
    "n_toplevel_user_comments": "Nutzerkommentare (ohne Replies)",
    "n_user_replies": "Nutzerreplies",
    "viewCount": "Views",
    "duration": "Videolänge",
    "mod_activity": "Moderationsaktivität",
    "responsivity": "Responsivität",
    "commentCount": "Kommentaranzahl",
    "replies_sentiment_mean": "Sentiment-Index (Replies)",
    "removed_comments_perc": "gelöschte Kommentare [%]",
    "comment_word_count": "Anzahl Kommentare (median)",
    "mean_word_count": "Mittlere Kommentarlänge",
    "comments_per_author": "Kommentare pro Autor",
    "videoCount": "Videoanzahl (gesamt)",
    "subscriberCount": "Abonennt*innen",
    "available_comments": "verfügb. Kommentare",
    "removed_comments": "gelöschte Kommentare",
    "comments_per_1kViews": "Kommentare pro 1000 Views",
    "categoryId": "YouTube Kategorie",
    "ratio_RepliesToplevel": "Reply Intensität (Reply vs Toplevel Kommentare)",
    "ZDF_content_references": "ZDFmediatheks Verweise",
    "references_per_video": "ZDFmediatheks Verweise pro Video",
    "toplevel_neutrality": "Neutralität",
}

# =============================================================================
# Plotly settings (include a button allowing select / deselect functionality)
# =============================================================================
px_select_deselect = dict(
    font=dict(size=18),
    updatemenus=[
        dict(
            type="buttons",
            direction="left",
            buttons=list(
                [
                    dict(
                        args=["visible", "legendonly"],
                        label="Deselect All",
                        method="restyle",
                    ),
                    dict(args=["visible", True], label="Select All", method="restyle"),
                ]
            ),
            pad={"r": 10, "t": 10},
            showactive=False,
            x=1,
            xanchor="right",
            y=1.1,
            yanchor="top",
        ),
    ],
)
